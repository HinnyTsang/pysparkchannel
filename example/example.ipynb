{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage on HPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to the spark session\n",
    "\n",
    "### 2. Download the source code by the procedure of [README.md](../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/hinny/data-science/pysparkchannel/example/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/HinnyTsang/pysparkchannel\n",
      " * branch            main       -> FETCH_HEAD\n",
      " * [new branch]      main       -> pysparkchannel/main\n",
      "warning: The following paths were already present and thus not updated despite sparse patterns:\n",
      "\tpysparkchannel/__init__.py\n",
      "\tpysparkchannel/core.py\n",
      "\tpysparkchannel/script.py\n",
      "\tpysparkchannel/utils.py\n",
      "\n",
      "After fixing the above paths, you may want to run `git sparse-checkout reapply`.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git init\n",
    "git remote add pysparkchannel https://github.com/HinnyTsang/pysparkchannel.git\n",
    "git config core.sparseCheckout true\n",
    "echo \"pysparkchannel\" > .git/info/sparse-checkout\n",
    "git pull pysparkchannel main\n",
    "\n",
    "rm -rf .git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the following cells\n",
    "\n",
    "The example will regenerate `module_a` and `module_b` in `dest` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing module: module_a\n",
      "|--  module_a\n",
      "   |__  __init__.py\n",
      "\n",
      "Parsing module: module_b\n",
      "|--  module_b\n",
      "   |--  sub_module_i\n",
      "      |__  sub_module_ii.py\n",
      "      |__  __init__.py\n",
      "   |__  module_bi.py\n",
      "   |__  __init__.py\n"
     ]
    }
   ],
   "source": [
    "# %%local\n",
    "# Run this cell in local\n",
    "\n",
    "# Import the pysparkchannel module\n",
    "import os\n",
    "import shutil\n",
    "from pysparkchannel.core import ModuleParser\n",
    "\n",
    "# Remove file if exist\n",
    "if os.path.exists(\"dest\"):\n",
    "    shutil.rmtree(\"dest\", ignore_errors=True)\n",
    "\n",
    "# Create the parser\n",
    "parser = ModuleParser(verbose = True)\n",
    "\n",
    "# Parse the custom module one by one.\n",
    "parser \\\n",
    "    .parse_module(\"module_a\") \\\n",
    "    .parse_module(\"module_b\")\n",
    "\n",
    "# Generate script to rewrite the module on spark cluster\n",
    "script = parser.generate_script(\"dest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use spark magic to copy the script to the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%send_to_spark -i script -t str -n script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the script on the spark session to regenerate the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing folder dest/module_a\n",
      "Reconstructing file dest/module_a/__init__.py\n",
      "Reconstructing folder dest/module_b\n",
      "Reconstructing folder dest/module_b/sub_module_i\n",
      "Reconstructing file dest/module_b/sub_module_i/sub_module_ii.py\n",
      "Reconstructing file dest/module_b/sub_module_i/__init__.py\n",
      "Reconstructing file dest/module_b/module_bi.py\n",
      "Reconstructing file dest/module_b/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# %%spark\n",
    "\n",
    "# Execute the script to rebuild the modules\n",
    "exec(script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32a3b9ca4b076343ba6d70f9e249a2bd7929cce5ce18abd3de2b1f8b52c74659"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
